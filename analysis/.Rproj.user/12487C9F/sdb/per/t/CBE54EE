{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Look at SUCCOTASH estimates when vary regularization parameter.\"\nauthor: \"David Gerard\"\n---\n\n**Last updated:** `r Sys.Date()`\n\n**Code version:** `r system(\"git log -1 --format='%H'\", intern = TRUE)`\n\n```{r chunk-options, include=FALSE}\nsource(\"chunk-options.R\")\n```\n\n## Abstract\nI look at an example where SUCCOTASH performs very poorly in estimating $\\pi_0$. Even when the regularization parameter is huge, SUCCOTASH will still underestimate $\\pi_0$. If we provide the true grid to SUCCOTASH, then it performs better.\n\n## Generate Data\n\nLoad in `succotashr` and source datamaker_gerard.R.\n```{r, echo = FALSE, results = \"hide\", message = FALSE, cache = TRUE}\nlibrary(succotashr)\nsource(\"../code/datamaker_gerard.R\")\n```\n\nSet parameters for data generation and obtain data. We'll look at the situation where\n\n* $p = 1000$,\n* $n = 20$ (10 vs 10)\n* $\\pi_0 = 0.9$\n* Data drawn from the Lung GTEX data and signal added via Poisson thinning.\n```{r, cache = TRUE, warning = FALSE, results = \"hide\", message=FALSE}\nset.seed(34789)\nargs_val <- list()\nargs_val$tissue <- \"Lung\"\nargs_val$path <- \"../data/\"\nargs_val$Ngene <- 1000\n## Nonnull case\nargs_val$poisthin <- TRUE\nargs_val$log2foldmean <- 0\nargs_val$skip_gene <- 5\nargs_val$Nsamp <- 10\nargs_val$log2foldsd <- 1\nargs_val$nullpi <- 0.9 ## true pi_0\n\nd_out <- datamaker(args_val)\n```\n\nExtract the data we need to run `succotash`.\n```{r}\nnum_sv <- d_out$meta$dfargs$num.sv\nYcounts <- d_out$input$counts\nY <- t(log(as.matrix(Ycounts) + 1)) ## log(counts + 1)\nX <- model.matrix(~d_out$input$condition)\n```\n\n## Run `succotash` and observe $\\hat{\\pi}_0$ values.\n\n```{r, results = 'hide', cache = TRUE}\nsuc_out <- succotash(Y = Y, X = X, k = num_sv, fa_method = \"pca\", lambda0 = 1, num_em_runs = 3) ## no regularization\nsuc_out10 <- succotash(Y = Y, X = X, k = num_sv, fa_method = \"pca\", lambda0 = 10, num_em_runs = 3) ## no regularization\nsuc_out100 <- succotash(Y = Y, X = X, k = num_sv, fa_method = \"pca\", lambda0 = 100, num_em_runs = 3) ## no regularization\nsuc_out200 <- succotash(Y = Y, X = X, k = num_sv, fa_method = \"pca\", lambda0 = 200, num_em_runs = 3) ## no regularization\n```\n\nThis is an example where the default regularization provides a $\\pi_0$ of only `r suc_out$pi_vals[1]`. Let's look at the estimates of the $\\pi$'s for each value of regularization.\n```{r}\nplot(suc_out$tau_seq, suc_out$pi_vals, type = 'h', ylim = c(0,1), xlab = expression(tau^2),\n     ylab = expression(hat(pi)[0]), main = \"Lambda = 1\")\nabline(h = args_val$nullpi, col = 2, lwd = 2, lty = 2)\nlegend(\"right\", \"True pi_0\", col = 2, lwd = 2, lty = 2)\n\nplot(suc_out10$tau_seq, suc_out10$pi_vals, type = 'h', ylim = c(0,1), xlab = expression(tau^2),\n     ylab = expression(hat(pi)[0]), main = \"Lambda = 10\")\nabline(h = args_val$nullpi, col = 2, lwd = 2, lty = 2)\nlegend(\"right\", \"True pi_0\", col = 2, lwd = 2, lty = 2)\n\nplot(suc_out100$tau_seq, suc_out100$pi_vals, type = 'h', ylim = c(0,1), xlab = expression(tau^2),\n     ylab = expression(hat(pi)[0]), main = \"Lambda = 100\")\nabline(h = args_val$nullpi, col = 2, lwd = 2, lty = 2)\nlegend(\"right\", \"True pi_0\", col = 2, lwd = 2, lty = 2)\n\nplot(suc_out200$tau_seq, suc_out200$pi_vals, type = 'h', ylim = c(0,1), xlab = expression(tau^2),\n     ylab = expression(hat(pi)[0]), main = \"Lambda = 200\")\nabline(h = args_val$nullpi, col = 2, lwd = 2, lty = 2)\nlegend(\"right\", \"True pi_0\", col = 2, lwd = 2, lty = 2)\n```\n\n\nA huge amount of mass is very close to 0, but not quite 0. When there is no regularization ($\\lambda = 1$), the amount\nof mass on variances less than 0.5 is `r sum(suc_out$pi_vals[suc_out$tau_seq < 0.5])`. I don't think I need to run more than 3 EM runs. By default, the first EM run puts $1/p$ mass on all $\\tau_k$ such that $\\pi_k \\neq 0$ and puts the remaining mass on $\\pi_0$. This would be an initial condition of placing `r 1 - length(suc_out$pi_vals) / args_val$Ngene` mass on $\\pi_0$, which is very large and close to the truth. If we give it the true grid but randomize the inital values of $\\pi_0$ and $\\pi_1$ then we get pretty good results:\n\n```{r}\nfor(index in 1:5) {\n  pi_init <- runif(1)\n  suc_out_zero <- succotashr::succotash_given_alpha(Y = suc_out$Y1_scaled, alpha = suc_out$alpha_scaled, \n                                      sig_diag = suc_out$sig_diag_scaled, num_em_runs = 10, \n                                      tau_seq = c(0, 1), em_pi_init = c(pi_init, 1 - pi_init))\n  cat(\"Repetition =\", index,\"\\n\")\n  cat(\"   Pi0_hat =\", suc_out_zero$pi_vals,\"\\n\\n\")\n}\n```\n\n\n## Session information\n\n```{r info}\nsessionInfo()\n```\n",
    "created" : 1456520307437.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1598875026",
    "id" : "CBE54EE",
    "lastKnownWriteTime" : 1456524354,
    "last_content_update" : 1456524354621,
    "path" : "~/Code/succotash_sims/analysis/succotash_ests.Rmd",
    "project_path" : "succotash_ests.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}